  <div class="row section_heading">
    <h2 align="center"><a href="#projects">Projects</a></h2>

    <div class="row">
      <div class="col-md-7">
        <h3 class="content_heading"><a target="_blank" href="http://evalai.cloudcv.org/">EvalAI</a></h3>
          <!-- <p class="">I worked under the guidance of <a href="https://filebox.ece.vt.edu/~parikh/">Dr. Devi Parikh</a> and <a href="https://filebox.ece.vt.edu/~dbatra/">Dr. Dhruv Batra</a>.</p>  -->
          <p class="">EvalAI is an open-source project under CloudCV. The aim is to build an open-source web platform enabling researchers to organize and participate in competitions to push state-of-the-art performance on Artificial Intelligence challenges. These challenges and benchmarks are essential for accurate evaluation of algorithms for a particular challenge. As the number of public datasets increase and more algorithms are published, it is becoming increasingly difficult to compare different approaches for a particular task. By providing a robust, scalable and a highly configurable platform, it is easy to create challenges with your own dataset,  evaluation protocol and a leaderboard. EvalAI shall avoid the need to run many baselines on several different datasets and makes it easier for researchers to perform exhaustive quantitative analysis in an efficient, reliable way. </p>
          <!-- <p class=""><b>ArXiV Link : </b><a href="https://arxiv.org/pdf/1604.03505v2.pdf">https://arxiv.org/pdf/1604.03505v2.pdf</a></p> -->
      </div>

      <div class="col-md-5" align="center">
        <iframe width="100%" height="315" src="https://www.youtube.com/embed/gf8NnxK2B24" frameborder="0" allowfullscreen></iframe>
      </div>
    </div>

    <div class="row">
      <div class="col-md-7">
        <h3 class="content_heading"><a target="_blank" href="http://evalai.cloudcv.org/">Theory of AI's Mind</a></h3>
          <!-- <p class="">I worked under the guidance of <a href="https://filebox.ece.vt.edu/~parikh/">Dr. Devi Parikh</a> and <a href="https://filebox.ece.vt.edu/~dbatra/">Dr. Dhruv Batra</a>.</p>  -->
          <p class="">Theory of Mind is the ability to attribute mental states (beliefs, intents, knowledge, perspectives, etc.) to others and recognize that these mental states may differ from one's own. It is critical to effective communication and to teams demonstrating higher collective performance. To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. Traditionally, the emphasis has been to make AI more accurate, and (to a lesser extent) on having it better understand human intentions, tendencies, beliefs, and contexts. The latter involves making AI more human-like and having it develop a theory of our minds. We argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind - get to know its strengths, weaknesses, beliefs, and quirks. We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples (50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. Surprisingly, we find that having access to the model's internal states - its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image - do not help people better predict its behavior</p>
          <p class=""><b>ArXiV Link : </b><a href="https://arxiv.org/pdf/1704.00717.pdf">https://arxiv.org/pdf/1704.00717.pdf</a></p>
          <p class=""><b>Tasks : </b><a href="https://www.youtube.com/watch?v=Dcs7GOmTAns">Failure Prediction</a> and <a href="https://www.youtube.com/watch?v=f_1ikwCuG4Q">Knowledge Prediction</a></p>   
      </div>

      <div class="col-md-5" align="center">
        <img src="https://media.giphy.com/media/xUA7bc8XxUUpVP8tPy/giphy.gif" width="100%" style="margin-top: 100px;">
      </div>
    </div>


    <div class="row">
      <div class="col-md-7">
        <h3 class="content_heading"><a target="_blank" href="https://filebox.ece.vt.edu/~prithv1/counting.html">Counting Everyday Objects in Everyday Scenes</a></h3>
          <p class="">I worked under the guidance of <a href="https://filebox.ece.vt.edu/~parikh/">Dr. Devi Parikh</a> and <a href="https://filebox.ece.vt.edu/~dbatra/">Dr. Dhruv Batra</a>.</p> 
          <p class="">We are interested in counting the numerosity of object classes in natural, everyday images. It is desirable to have agents that can count objects in real-life scenarios. Previous counting approaches tackle the problem in restricted domains such as counting pedestrians in surveillance videos. Counts can also be estimated from outputs of other vision tasks like object detection. In contrast, we build dedicated models for counting designed to tackle the large variance in counts, appearances, and scales of objects found in natural scenes. Our approach is inspired by the phenomenon of subitizing - the ability of humans to make quick assessments of counts given a perceptual signal, for small count values. Given a natural scene, we employ a divide and conquer strategy while incorporating context across the scene to adapt the subitizing idea to counting. We call this Sequential Subitizing. Our approach offers consistent improvements over numerous baseline approaches for counting on the PASCAL VOC 2007 and COCO datasets. Subsequently, we explore how counting can be used to improve the performance of object detectors. We then show a proof of concept application of our counting methods to the task of Visual Question Answering, by studying the `how many?' questions in the VQA and COCO-QA datasets.</p>
          <p class=""><b>ArXiV Link : </b><a href="https://arxiv.org/pdf/1604.03505v2.pdf">https://arxiv.org/pdf/1604.03505v2.pdf</a></p>
          <p class=""><b>Spotlight : </b><a href="http://cvpr2017.thecvf.com/">CVPR 2017</a></p>
      </div>

      <div class="col-md-5" align="center">
        <img src="static/img/seq_new.png" width="100%" style="margin-top: 100px;">
      </div>
    </div>

    <div class="row">
      <div class="col-md-7">
        <h3 class="content_heading"><a target="_blank" href="http://robotics.iiit.ac.in/uploads/Main/Publications/Siva_etal_ICVGIP_14.pdf">Guess from Far, Recognize when Near : Searching the Floor for Small Objects</a></h3>
          <p class="">I worked under the guidance of <a href="http://faculty.iiit.ac.in/~mkrishna/">Dr. K. Madhava Krishna</a>.</p> 
          <p class="">We implemented an efficient strategy for a robot to explore, discover, recognize and navigate to a selected few objects among a number of objects scattered on the floor, based on guess from far and recognize from near strategy. From far away, we assign Existential Probabilities to the objects, indicating their similarity to queried objects. A Bayes’ Net is constructed over the probabilities, to overlay and orient a Viewpoint Object Potential(VOP) map over potential search objects. VOP quantifies the probability of accurately recognizing an object through its RGB-D Point Cloud at various viewpoints. Further a decision tree approach is used to formulate a optimal control plan. The framework has been tested on a kinect mounted on a robotic platform. Development in ROS, C++.</p>
      </div>

      <div class="col-md-5" align="center">
        <img src="static/img/iiit_proj.png" width="100%" style="margin-top: 100px;">
      </div>
    </div>

    <div class="row">
      <div class="col-md-7">
        <h3 class="content_heading">Charged Black holes in Constant Scalar Curvature f(R) Gravity</h3>
          <p class="">I worked under the guidance of <a href="http://mailweb.iacs.res.in/theoph/tpssg/">Dr. Soumitra Sengupta</a>.</p> 
          <p class="">My specific focus was looking for Charged Rotating Black Hole Solutions in Einstein-Gauss-Bonnet Dilaton Coupled Gravity. I got to explore different theories of gravity attempting to unify Quantum Mechanics and General Relativity. I studied and simulated the conditions for existence of multiple horizons in constant scalar curvature f(R) gravity and acquired results demonstrating the convergence of event and cosmological horizon. As an interesting exercise, I also studied an axisymmetric black hole in constant scalar curvature f(R) gravity</p>
          <p class=""><b>Report : </b><a href="https://drive.google.com/file/d/0BxMpUrO9U5eCSlBqaXJwWFMwU00/view?usp=sharing">Findings</a></p>
      </div>

      <div class="col-md-5" align="center">
        <img src="static/img/GR.png" width="90%" style="margin-top: 100px;">
      </div>
    </div>

    <div class="row">
      <div class="col-md-7">
        <h3 class="content_heading"><a target="_blank" href="https://www.facebook.com/AUV.DTU/">Autonomous Underwater Vehicle</a></h3>
          <p class="">I worked under the guidance of <a href="http://spie.org/profile/rsinha">Dr. R.K. Sinha</a>. As a part of the team, I worked in the underwater acoustics and control systems department.</p>
          <p class=""><b> Underwater Acoustics</b> - My aim was to develop a Passive SONAR system for 3D localization of a sound source underwater. I studied the feasibility of solutions of the hyperboloid formulation of the localization problem and developed and Implemented two Range Estimation Algorithms using TDOA (Time Difference of Arrival) Estimations. I also worked on the implementation of the method of spherical intersection for localization of sound sources. I developed and simulated an algorithm for localization in cylindrical coordinates using TDOA and Bearing Estimations.</a></p>
          <p class=""><b>Control Systems</b> - My aim was to develop a robust control algorithm for the Autonomous Underwater Vehicle. I worked on developing the Control Module to implement simultaneous PID loops to maintain the orientation of the AUV and developed GUIs to tune and adjust the PID parameters accordingly. On the technical front, I implemented the use of an Android phone as the IMU (Inertial Measurement Unit) using an application for Android operating System. The Android phone uses the UDP communication for sending and receiving data packets between itself and the single board computer.</p>
          <p class=""><b>Journal Paper : </b><a href="http://www.auvsifoundation.org/sites/default/files/Delhi_Tech_2014_RoboSub_Journal.pdf">AUVSI Robosub Journal DTU-AUV</a></p>
      </div>

      <div class="col-md-5" align="center">
        <img src="static/img/zyra.jpg" width="100%" style="margin-top: 100px;">
      </div>
    </div>

    <div class="row">
      <div class="col-md-7">
        <h3 class="content_heading">Evaluating VQA Models</a></h3>
          <p class=""><b>Undergraduate Major Project</b> under the guidance of <a href="http://www.dtu.ac.in/Web/Departments/Electrical/faculty/">Dr. Mini Sreejeth.</a></p>
          <p class="">I implemented several baseline Visual Question Answering (VQA) models and compared their performances.</p>
          <p class="">I started off building models to answerthe ’how many?’ questions and implemented a DeepDream-based qualitative experiment using GoogleNet to study the compositionality characteristics of counting models. Given a base image, used the DeepDream framework to generate images for a given class and studied the count variations of related classes</p>
          <p class="">Implemented the VQA - LSTM + CNN baseline model, Hierarchical Co-attention and subsequently the then state-of-the-art Multimodal Compact Bilinear Pooling VQA and Visual Grounding model in torch and prepared demonstrations using Django and PyTorch</p>
      </div>

      <div class="col-md-5" align="center">
        <img src="static/img/vqa.png" width="100%" style="margin-top: 100px;">
      </div>
    </div>

    <div class="row">
      <div class="col-md-7">
        <h3 class="content_heading">Multi-stage Scene Understanding Pipeline</a></h3>
          <p class=""><b>Undergraduate Minor Project</b> under the guidance of <a href="http://www.dtu.ac.in/Web/Departments/Electrical/faculty/">Dr. Mini Sreejeth.</a></p>
          <p class="">I implemented a multi-stage pipeline for scene understanding using Scene Classification and Video Magnification Techniques (Setup from CSAIL MIT) as a minor project during coursework. Different Stages included basic scene classification techniques using GIST features followed by superpixeling to detect and identify objects and using video magnification to study subtle movements</p>
      </div>

      <div class="col-md-5" align="center">
        <img src="static/img/surveillance.jpg" width="60%" style="margin-top: 60px;">
      </div>
    </div>
  </div>
