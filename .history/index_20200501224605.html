<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- DELETE THIS SCRIPT if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Prithvijit Chattopadhyay</title>
  
  <meta name="author" content="Prithvijit Chattopadhyay">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Prithvijit Chattopadhyay</name>
              </p>
              <p>
              <!-- <strong><font color="blue">I'll be starting as a computer science PhD student at Georgia Tech in Fall 2019.</font></strong>  -->
              I am a 1<sup>st</sup> year CS PhD
              student at Georgia Tech, advised by <a href="https://www.cc.gatech.edu/~judy/">Prof. Judy Hoffman</a>. I also closely collaborate with <a href="https://filebox.ece.vt.edu/~parikh/">Prof. Devi Parikh</a> and <a href="https://filebox.ece.vt.edu/~dbatra/">Prof. Dhruv Batra</a>. 
              I recently earned 
              my Masters in Computer Science (focus on Machine Learning) from Georgia Tech, advised by <a href="https://filebox.ece.vt.edu/~parikh/">Prof. Devi Parikh</a>. 
              <!-- I also work closely with <a href="https://filebox.ece.vt.edu/~dbatra/">Prof. Dhruv Batra</a>. -->
              Prior to joining Georgia Tech, I was working as a Research Assistant in the Computer Vision Machine Learning and Perception Lab (CVMLP) at Virginia Tech, advised by <a href="https://filebox.ece.vt.edu/~parikh/">Prof. Devi Parikh</a> and <a href="https://filebox.ece.vt.edu/~dbatra/">Prof. Dhruv Batra</a>. I earned my Bachelors in Electrical Engineering in 2016 from Delhi Technological University, India.
              </p>

              <p class="content">In the past couple of years, I have had the fortune to intern / conduct research at <a href="https://www.microsoft.com/en-us/research/group/deep-learning-group/">Deep Learning Group, Microsoft Research Redmond</a> (Summer 2018) mentored by <a href="https://www.microsoft.com/en-us/research/people/hpalangi/">Hamid Palangi</a>; <a href="http://robotics.iiit.ac.in/">Robotics Research Lab, IIIT Hyderabad</a> (Winter 2014) mentored by <a href="http://faculty.iiit.ac.in/~mkrishna/">Dr. K. Madhava Krishna</a> and <a href="http://www.iacs.res.in/">Indian Association for the Cultivation of Science (IACS), Kolkata</a> (Summer 2014) mentored by <a href="http://mailweb.iacs.res.in/theoph/tpssg/">Dr. Soumitra Sengupta</a> on a diverse set of topics - ranging from vision & language to robotics to theoretical physics.</p>

              <!-- <p class="content">Previously, I have worked with <a href="http://faculty.iiit.ac.in/~mkrishna/">Dr. K. Madhava Krishna</a> on problems relating to Robot Vision and Navigation at Robotics Research Lab, IIIT Hyderabad. My interest in robotics stemmed from being a part of Autonomous Underwater Vehicle Team in my undergrad days. I specifically worked on Underwater Acoustics and Control System problems relating to the bot.</p>

              <p class="content">I also find Theoretical Physics to be fascinating. Iâ€™ve had the privilege to work with <a href="http://mailweb.iacs.res.in/theoph/tpssg/">Dr. Soumitra Sengupta</a> on f(R) gravity. Our quasi-static journey towards a unified theory, joining Quantum Mechanics and General Relativity, is definitely a rewarding one. My interest in this specific branch of physics originated from trying to study Differential Geometry in my early undergrad days.</p>  -->

              <p class="content">I occasionally play the percussion instrument Tabla. I am very passionate about movies. I love to break them down shot-by-shot and analyze them. Every single frame is important.</p>

              <p style="text-align:center">
                <a href="mailto:prithvijit3@gatech.edu">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=rIK7AMkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/prithvijit-chattopadhyay-260b2b54/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://github.com/prithv1"> Github </a> &nbsp/&nbsp
                <a href="https://twitter.com/prithvijitch"> Twitter </a> &nbsp/&nbsp
                <a href="https://www.instagram.com/prithv12/?hl=en"> Instagram </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/prithv1.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/prithv1-circ.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                <!-- I am broadly interested in problems at the intersection of Computer Vision and Natural Language Processing. -->
                The problems that I work on generally lie at the intersection of Computer Vision and Natural Language Processing.
                More specifically, I am interested in developing intelligent systems (including applications of Machine Learning and
                Reinforcement Learning) that
                <!-- The problems that I work on lie at the intersection of Computer Vision, Machine Learning and Natural Language Processing. Specifically, I am interested in developing AI systems that -->
              </p>
              <p class="content">
              <ul>
                <li>can <b><i>perceive and reason</i></b> based on multimodal sensory information</li>
                <li>are <b><i>interpretable</i></b> so that predictions made by such systems can be explained</li>
                <li>are <b><i>transferable</i></b> so that they can be adapted across different domains with ease and limited supervision</li>
              </ul>
              </p>
              <p> 
                Representative papers are <span class="highlight">listed under Papers</span>.
              </p>

            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Achievements</heading>
              <ul>
                <li>Awarded the College of Computing's <a href="https://www.cc.gatech.edu/content/2020-gt-computing-student-awards">CS7001 Research Award</a> at Georgia Tech!</li>
                <li>Recognized as one of the highest-scoring reviewers for NeurIPS 2019!</li>
                <li>Recognized as an <a href="https://iclr.cc/Conferences/2019/Awards">outstanding reviewer</a> for ICLR 2019!</li>
                <li>Recognized to be among the top 20 percent highest scoring reviewers for NeurIPS 2018!</li>
                <li>Awarded the College of Computing's MS Research Award at Georgia Tech!</li>
                <li>Our team won <a href="http://www.vthacks.com/">VT Hacks 2017</a>, a <a href="https://mlh.io/"> Major League Hacking Event</a>, 2017!</li>
                <li>Our undergraduate team, DTU-AUV, qualified for the semi-finals at <a href="http://www.auvsifoundation.org/2014-robosub-teams">AUVSI Robosub 2014</a>!</li>
                <li>Awarded Merit Scholarships from 2012-2014 for undergraduate academic performance!</li>
                <li>Selected for <a href="http://kvpy.iisc.ernet.in/main/index.htm">KVPY</a> and <a href="http://www.inspire-dst.gov.in/fellowship.html">INSPIRE</a> Fellowships, 2012 for undergraduate studies in basic sciences!</li>
                <li>Placed among the top 1 percent students in the country in <a href="http://www.indapt.org/index.php/inphoipho">INPhO</a> 2012!</li>
                <li>Selected for rigorous mathematical training camps conducted by mathematicians from Bhabha Atomic Research Center (<a href="http://www.barc.gov.in/">BARC</a>) and Indian Institute of Science (<a href="http://www.iisc.ac.in/">IISc</a>) in 2012!</li>
                <li>Selected for <a href="http://www.csirhrdg.res.in/cpyls.htm">CSIR Programme on Youth Leadership in Science</a>, 2010!</li>

              </ul>
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>News</heading>
              <ul>
                <li>May 2020: I'll be interning in the <a href="https://prior.allenai.org/">PRIOR Team</a> at Allen Institute of Artificial Intelligence.</li>
                <li> Serving as a reviewer for <a href="http://cvpr2018.thecvf.com/">CVPR 2018</a>, <a href="https://eccv2018.org/">ECCV 2018</a>, <a href="https://nips.cc/">NeurIPS 2018-19-20</a>, <a href="https://iclr.cc/">ICLR 2019-20</a>, <a href="https://icml.cc/Conferences/2020">ICML 2019-20</a>, <a href="http://www.acl2019.org/EN/index.xhtml">ACL 2019</a>.</li>
                <li>May 2019: Presented our work on Discovery of Decision States through Intrinsic Control at the <a href="https://tarl2019.github.io">Task-Agnostic Reinforcement Learning (TARL) Workshop</a> at ICLR 2019.</li>
                <li>May 2019:<font color="red"><strong> Completed my Masters in Computer Science (focus on Machine Learning)</font> </strong> with a thesis centered on <a href="https://smartech.gatech.edu/handle/1853/61308">Evaluating Visual Conversational Agents in the Context of Human-AI Cooperative Games</a>!</li>
                <li>Feb 2019: Our technical report describing <a href="http://evalai.cloudcv.org/">EvalAI</a> - an open source platform to evaluate and compare AI algorithms at scale - is out on <a href="https://arxiv.org/pdf/1902.03570.pdf"> ArXiv</a>!</li>
                <li>Dec 2018: Presented our work <a href="https://arxiv.org/abs/1808.02861">interpretable zero-shot learning</a> at the <a href="https://sites.google.com/view/continual2018/home?authuser=0">Continual Learning </a> and <a href="https://nips2018vigil.github.io/">Visually Grounded Interaction and Language (ViGIL)</a> workshops at NeurIPS 2018.</li>
                <li>Aug 2018: Our paper titled <a href="https://arxiv.org/pdf/1810.12366.pdf">'Do explanation modalities make VQA models more predictable to a human?'</a></a> was accepted in <a href="http://emnlp2018.org/">EMNLP, 2018</a>!</li>
                <li>Jul 2018: Our paper titled <a href="https://arxiv.org/abs/1808.02861">'Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance'</a> was accepted in <a href="https://eccv2018.org/">ECCV, 2018</a>!</li>
                <li>Apr 2018: I was awarded the College of Computing's MS Research Award at Georgia Tech!</li>
                <li>Feb 2018: I'll intern in the Deep Learning Group at <a href="https://www.microsoft.com/en-us/research/group/deep-learning-group/">Microsoft Research, Redmond</a> in summer 2018.</li>
                <li>Aug 2017: Our paper titled <a href="https://arxiv.org/abs/1708.05122">'Evaluating Visual Conversational Agents via Cooperative Human-AI Games'</a> was accepted in <a href="https://www.humancomputation.com/2017/">HCOMP 2017</a> as an oral!</li>
                <li>Jul 2017: I will be joining Georgia Tech as a Masters of Computer Science student in Fall 2017.</li>
                <li>May 2017: I will be presenting <a href="https://arxiv.org/abs/1604.03505">'Counting Everyday Objects in Everyday Scenes'</a> at the <a href="http://www.ldv.co/visionsummit/">LDV Vision Summit, 2017</a>.</li>
                <li>Mar 2017: Our paper titled <a href="https://arxiv.org/abs/1704.00717">'It Takes Two to Tango: Towards Theory of AI's Mind'</a> is out on ArXiv!</li>
                <li>Feb 2017: Our paper titled <a href="https://arxiv.org/abs/1604.03505">'Counting Everyday Objects in Everyday Scenes'</a> was accepted in <a href="http://cvpr2017.thecvf.com/"> CVPR 2017 </a> as a spotlight!</li>
                <li>Feb 2017: Our team built <a href="https://devpost.com/software/filterai">FilterAI</a> - an image retrival engine - and won <a href="http://www.vthacks.com/">VT Hacks 2017</a>, a <a href="https://mlh.io/"> Major League Hacking Event</a>!</li>
                <li>Dec 2016: <a href="https://arxiv.org/abs/1604.03505">'Counting Everyday Objects in Everyday Scenes'</a></a> received an <a href="https://aws.amazon.com/blogs/publicsector/call-for-computer-vision-research-proposals-with-new-amazon-bin-image-data-set/"> Amazon Academic Research Award</a>, 2016!</li>
              </ul>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Papers</heading> (<sup>*</sup> joint first authors)
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/visdial_div_teaser.jpg" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1909.10470">
                      <papertitle>Improving Generative Visual Dialog by Answering Diverse Questions
</papertitle>
              </a>
            <br>
            Vishvak Murahari,
            <strong><u>Prithvijit Chattopadhyay</u></strong>,
            Dhruv Batra,
            Devi Parikh,
            Abhishek Das
            <br>
              <em>EMNLP</em>, 2019 <font color="red">(Poster)</font>; <a href="https://visualqa.org/workshop.html"><em>Visual Question Answering and Dialog Workshop, CVPR</em> 2019</a> <font color="red">(Poster)</font>
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://arxiv.org/abs/1909.10470">arxiv</a> /
              <a href="https://github.com/vmurahari3/visdial-diversity">code</a>
              <p align="justify">While generative visual dialog models trained with self-talk based RL perform better at the associated downstream task, they suffer from repeated interactions -- resulting in saturation in improvements as the number of rounds increase. To counter this, we devise a simple auxiliary objective that incentivizes Q-Bot to ask diverse questions, thus reducing repetitions and in turn enabling A-Bot to explore a larger state space during RL i.e., be exposed to more visual concepts to talk about, and varied questions to answer.</p> 
            </td>
          </tr>


          <tr>
            <td width="30%"><img src="images/ds_vic_new.png" alt="3DSP" width="230" height="200" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1907.10580">
                      <papertitle>DS-VIC: Unsupervised Discovery of Decision States for Transfer in RL
</papertitle>
              </a>
            <br>
            Nirbhay Modhe,
            <strong><u>Prithvijit Chattopadhyay</u></strong>,
            Mohit Sharma,
            Abhishek Das,
            Devi Parikh,
            Dhruv Batra,
            Ramakrishna Vedantam
            <br>
              <em>arxiv Preprint</em>, 2019; <a href="https://tarl2019.github.io/"><em>Task-Agnostic RL (TARL) Workshop, ICLR</em> 2019</a> <font color="red">(Poster)</font><br>
              <a href="https://arxiv.org/abs/1907.10580">arxiv</a> /
              <a href="https://tarl2019.github.io/assets/papers/modhe2019unsupervised.pdf">TARL'19 Preliminary Version</a><br>
              <a href="https://tarl2019.github.io/assets/papers/modhe2019unsupervised.pdf">(Revised version accepted in IJCAI 2020)</a>
              <p align="justify">We learn to identify decision states, namely the parsimonious set of states where decisions meaningfully affect the future states an agent can reach in an environment. We utilize the VIC framework, which maximizes an agent's 'empowerment', i.e. the ability to reliably reach a diverse set of states -- and formulate a sandwich bound on the empowerment objective that allows identification of <i>decision states</i>. Unlike previous work, our decision states are discovered without extrinsic rewards -- simply by interacting with the world. Our results show that our decision states are: (1) often interpretable, and (2) lead to better exploration on downstream goal-driven tasks in partially observable environments.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/evalai_teaser.png" alt="3DSP" width="220" height="180" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/pdf/1902.03570">
                      <papertitle>EvalAI: Towards Better Evaluation Systems for AI Agents</papertitle>
              </a>
            <br>
            Deshraj Yadav,
            Rishabh Jain,
            Harsh Agrawal,
            <strong><u>Prithvijit Chattopadhyay</u></strong>,
            Taranjeet Singh,
            Akash Jain,
            Shiv Baran Singh,
            Stefan Lee,
            Dhruv Batra
            <br>
              <em>arxiv Preprint</em>, 2019<br>
              <a href="https://arxiv.org/abs/1902.03570">arxiv</a> /
              <a href="https://github.com/Cloud-CV/EvalAI">code</a>
              <p align="justify">We introduce EvalAI, an open source platform for evaluating and comparing machine learning (ML) and artificial intelligence algorithms (AI) at scale. EvalAI is built to provide a scalable solution to the research community to fulfill the critical need of evaluating machine learning models and agents acting in an environment against annotations or with a human-in-the-loop. This will help researchers, students, and data scientists to create, collaborate, and participate in AI challenges organized around the globe.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/NIWT_teaser.png" alt="3DSP" width="240" height="200" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1808.02861">
                      <papertitle>Choose Your Neuron: Incorporating Domain-Knowledge through Neuron-Importance</papertitle>
              </a>
            <br>
            Ramprasaath R. Selvaraju<sup>*</sup>,
            <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
            Mohamed Elhoseiny,
            Tilak Sharma,
            Dhruv Batra,
            Devi Parikh,
            Stefan Lee
            <br>
              <em>ECCV</em>, 2018 <font color="red">(Poster)</font>; <a href="https://sites.google.com/view/continual2018/home?authuser=0"><em>Continual Learning Workshop, NeurIPS</em> 2018</a> <font color="red">(Poster)</font>; <a href="https://nips2018vigil.github.io/"><em>Visually Grounded Interaction and Language (ViGIL) Workshop, NeurIPS</em> 2018</a> <font color="red">(Poster)</font><br>
              <a href="https://arxiv.org/abs/1808.02861">arxiv</a> /
              <a href="https://mlatgt.blog/2018/09/05/choose-your-neuron-incorporating-domain-knowledge-through-neuron-importance/">blogpost</a> /
              <a href="https://github.com/ramprs/neuron-importance-zsl">code</a>
              <p align="justify"> We introduce a simple, efficient zero-shot learning approach -- NIWT -- based on the observation that individual neurons in CNNs have been shown to implicitly learn a dictionary of semantically meaningful concepts (simple textures and shapes to whole or partial objects). NIWT learns to map domain knowledge about "unseen" classes onto this dictionary of learned concepts and optimizes for network parameters that can effectively combine these concepts - essentially learning classifiers by discovering and composing learned semantic concepts in deep networks.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/explanations_predictable.png" alt="3DSP" width="205" height="200" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1810.12366">
                      <papertitle>Do explanation modalities make VQA Models more predictable to a human?</papertitle>
              </a>
            <br>
            Arjun Chandrasekaran<sup>*</sup>,
            Viraj Prabhu<sup>*</sup>,
            Deshraj Yadav<sup>*</sup>,
            <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
            Devi Parikh
            <br>
              <em>EMNLP</em>, 2018 <font color="red">(Poster)</font><br>
              <a href="https://arxiv.org/pdf/1810.12366">arxiv</a>
              <p align="justify">A rich line of research attempts to make deep neural networks more transparent by generating human-interpretable 'explanations' of their decision process, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model -- its responses as well as failures -- more predictable to a human.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/eval_visdial.png" alt="3DSP" width="180" height="220" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1708.05122">
                      <papertitle>Evaluating Visual Conversational Agents via Cooperative Human-AI Games</papertitle>
              </a>
            <br>
            <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
            Deshraj Yadav<sup>*</sup>,
            Viraj Prabhu, 
            Arjun Chandrasekaran,
            Abhishek Das,
            Stefan Lee,
            Dhruv Batra,
            Devi Parikh
            <br>
              <em>HCOMP</em>, 2017 <font color="red">(Oral)</font><br>
              <a href="https://arxiv.org/abs/1708.05122">arxiv</a> /
              <a href="https://github.com/GT-Vision-Lab/GuessWhich">code</a>
              <p align="justify">We design a cooperative game - GuessWhich - to measure human-AI team performance in the specific context of the AI being a visual conversational agent. GuessWhich involves live interaction between the human and the AI and is designed to gauge the extent to which progress in isolated metrics for AI (& AI-AI teams) transfers to human-AI collaborative scenarios.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/toaim.png" alt="3DSP" width="200" height="120" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1704.00717">
                      <papertitle>It Takes Two to Tango: Towards Theory of AI's Mind</papertitle>
              </a>
            <br>
            Arjun Chandrasekaranu<sup>*</sup>,
            Deshraj Yadav<sup>*</sup>,
            <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
            Viraj Prabhu<sup>*</sup>,
            Devi Parikh
            <br>
              <em>Chalearn Looking at People Workshop, CVPR</em>, 2017 <font color="red">(Oral)</font><br>
              <a href="https://arxiv.org/abs/1704.00717">arxiv</a> /
              <a href="https://github.com/deshraj/TOAIM">code</a>
              <p align="justify">To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. In this work, we argue that for human-AI teams to be effective, in addition to making AI more accurate and human-like, humans must also develop a theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs, and quirks. </p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/counting_task_img.jpg" alt="3DSP" width="230" height="140" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1604.03505">
                      <papertitle>Counting Everyday Objects in Everyday Scenes</papertitle>
              </a>
            <br>
            <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
            Ramakrishna Vedantam<sup>*</sup>,
            Ramprasaath R. Selvaraju,
            Dhruv Batra,
            Devi Parikh
            <br>
              <em>CVPR</em>, 2017 <font color="red">(Spotlight)</font><br>
              <a href="https://arxiv.org/abs/1604.03505">arxiv</a> /
              <a href="https://github.com/prithv1/cvpr2017_counting">code</a>
              <p align="justify">We study the numerosity of object classes in natural, everyday images and build dedicated models for counting designed to tackle the large variance in counts, appearances, and scales of objects found in natural scenes. We propose a contextual counting approach inspired by the phenomenon of subitizing - the ability of humans to make quick assessments of counts given a perceptual signal, for small count values.</p> </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
                  <hr>
                  <p align="center">
                  <font>(Design and CSS courtesy: <a href="https://jonbarron.info/">Jon Barron</a> and <a href="https://abhoi.github.io/">Amlaan Bhoi</a>)</font>
                  </p>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
